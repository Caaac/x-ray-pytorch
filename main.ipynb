{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b762cf0f",
   "metadata": {},
   "source": [
    "### Анализ рентгеновских снимков по картинкам с прогнозированием заболеваний\n",
    "\n",
    "#### [Ссылка на датасет](https://huggingface.co/datasets/Sohaibsoussi/NIH-Chest-X-ray-dataset-small)\n",
    "\n",
    "```\n",
    "class_label:\n",
    "  '0': No Finding\n",
    "  '1': Atelectasis\n",
    "  '2': Cardiomegaly\n",
    "  '3': Effusion\n",
    "  '4': Infiltration\n",
    "  '5': Mass\n",
    "  '6': Nodule\n",
    "  '7': Pneumonia\n",
    "  '8': Pneumothorax\n",
    "  '9': Consolidation\n",
    "  '10': Edema\n",
    "  '11': Emphysema\n",
    "  '12': Fibrosis\n",
    "  '13': Pleural_Thickening\n",
    "  '14': Hernia\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "195e0b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from datetime import date, datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms.v2 as tfs_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44a9fcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR_PATH = os.path.abspath(os.path.curdir)\n",
    "DATASET_DIR_PATH = os.path.join(PROJECT_DIR_PATH, 'dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99a4174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XRayDataset(data.Dataset):\n",
    "    def __init__(self, path: str, type_file: str, transform=None):\n",
    "        self.path = path\n",
    "        self.type_file = type_file\n",
    "        self.transform = transform\n",
    "\n",
    "        self.length = 0\n",
    "        # self.files = []\n",
    "        # self.targets = []\n",
    "        self.classes = torch.eye(15)\n",
    "\n",
    "        not_split_data = self.get_unpack_img()\n",
    "        files, targets, _length = self.split_data(not_split_data)\n",
    "\n",
    "        self.df = pd.DataFrame({'path': files, 'target': targets})\n",
    "        self.length = self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        file_path, target = self.df.iloc[index]\n",
    "        img = Image.open(file_path).convert('RGB')\n",
    "        \n",
    "        if (self.transform is not None):\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, self.classes[target]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def get_unpack_img(self) -> list:\n",
    "        type_file_path = os.path.join(self.path, self.type_file)\n",
    "\n",
    "        if os.path.exists(type_file_path):\n",
    "            shutil.rmtree(type_file_path)\n",
    "\n",
    "        os.makedirs(type_file_path, exist_ok=True)\n",
    "\n",
    "        parquet_files = [f for f in os.listdir(self.path) if f.startswith(\n",
    "            self.type_file) and f.endswith('.parquet')]\n",
    "\n",
    "        result = []\n",
    "\n",
    "        for file_name in tqdm(parquet_files, desc=f\"Unpacking {self.type_file}\"):\n",
    "            df = pd.read_parquet(os.path.join(self.path, file_name))\n",
    "\n",
    "            for idx, row in df.iterrows():\n",
    "                img_path = os.path.join(type_file_path, f\"image_{idx}.png\")\n",
    "\n",
    "                if isinstance(row['image']['bytes'], bytes):\n",
    "                    with open(img_path, 'wb') as f:\n",
    "                        f.write(row['image']['bytes'])\n",
    "                # else:\n",
    "                #     img = Image.fromarray(row['image'])\n",
    "                #     img.save(img_path)\n",
    "\n",
    "                result.append((img_path, row['labels']))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def split_data(self, not_split_data: list):\n",
    "        files = []\n",
    "        targets = []\n",
    "        length = 0\n",
    "\n",
    "        for path, labels in not_split_data:\n",
    "            for target in labels.tolist():\n",
    "                files.append(path)\n",
    "                targets.append(int(target))\n",
    "                length += 1\n",
    "\n",
    "        return files, targets, length\n",
    "\n",
    "    def cut_dataframe(self, target: str | int, count: int) -> None:\n",
    "        rows_to_drop = self.df[self.df['target'] == target].index[:count]\n",
    "        self.df = self.df.drop(rows_to_drop)\n",
    "        self.length = self.df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "214b4bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unpacking test: 100%|██████████| 2/2 [00:00<00:00,  2.96it/s]\n",
      "Unpacking train: 100%|██████████| 4/4 [00:02<00:00,  1.96it/s]\n",
      "Unpacking validation: 100%|██████████| 2/2 [00:00<00:00,  3.20it/s]\n"
     ]
    }
   ],
   "source": [
    "d_test = XRayDataset(DATASET_DIR_PATH, 'test')\n",
    "d_train = XRayDataset(DATASET_DIR_PATH, 'train')\n",
    "d_valid = XRayDataset(DATASET_DIR_PATH, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8f4007a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_DATASET\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5260 entries, 0 to 5259\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   path    5260 non-null   object\n",
      " 1   target  5260 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 82.3+ KB\n",
      "None\n",
      "\n",
      "target\n",
      "0     2489\n",
      "4      685\n",
      "3      459\n",
      "1      431\n",
      "6      243\n",
      "5      194\n",
      "9      165\n",
      "8      144\n",
      "13     108\n",
      "2       77\n",
      "12      75\n",
      "11      71\n",
      "10      61\n",
      "7       43\n",
      "14      15\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('TRAIN_DATASET')\n",
    "print(d_train.df.info(), end='\\n\\n')\n",
    "print(d_train.df.target.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06b28582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_DATASET\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1828 entries, 0 to 1827\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   path    1828 non-null   object\n",
      " 1   target  1828 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 28.7+ KB\n",
      "None\n",
      "\n",
      "target\n",
      "0     541\n",
      "4     284\n",
      "3     213\n",
      "1     160\n",
      "8     146\n",
      "5      96\n",
      "9      77\n",
      "6      73\n",
      "11     60\n",
      "13     55\n",
      "2      42\n",
      "10     31\n",
      "12     26\n",
      "7      23\n",
      "14      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('TEST_DATASET')\n",
    "print(d_test.df.info(), end='\\n\\n')\n",
    "print(d_test.df.target.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "387ccdb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0     1189\n",
       "4      685\n",
       "3      459\n",
       "1      431\n",
       "6      243\n",
       "5      194\n",
       "9      165\n",
       "8      144\n",
       "13     108\n",
       "2       77\n",
       "12      75\n",
       "11      71\n",
       "10      61\n",
       "7       43\n",
       "14      15\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_train.cut_dataframe(0, 1300)\n",
    "d_train.df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5b7a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/Dev/DS/x-ray-pytorch/venv/lib/python3.13/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "transform = tfs_v2.Compose([\n",
    "    tfs_v2.ToTensor(),\n",
    "    tfs_v2.ToDtype(dtype=torch.float32, scale=True)\n",
    "])\n",
    "\n",
    "d_train.transform = transform\n",
    "train_data = data.DataLoader(d_train, batch_size=4, shuffle=True)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 128, 3, padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Conv2d(128, 64, 3, padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Conv2d(64, 32, 3, padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Conv2d(32, 16, 3, padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    nn.Conv2d(16, 8, 3, padding='same'),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2),\n",
    "    # nn.Conv2d(8, 4, 3, padding='same'),\n",
    "    # nn.ReLU(),\n",
    "    # nn.MaxPool2d(2, 2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(8192, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024, 15)\n",
    ")\n",
    "\n",
    "# model = nn.Sequential(\n",
    "#   nn.Conv2d(3, 32, 3, padding='same'),\n",
    "#   nn.ReLU(),\n",
    "#   nn.MaxPool2d(2, 2),\n",
    "#   nn.Conv2d(32, 8, 3, padding='same'),\n",
    "# \tnn.ReLU(),\n",
    "# \tnn.MaxPool2d(2, 2),\n",
    "# \tnn.Conv2d(8, 4, 3, padding='same'),\n",
    "# \tnn.ReLU(),\n",
    "#  \tnn.MaxPool2d(2, 2),\n",
    "# \tnn.Flatten(),\n",
    "# \tnn.Linear(65536, 4096),\n",
    "# \tnn.ReLU(),\n",
    "# \tnn.Linear(4096, 15)\n",
    "# )\n",
    "\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "# loss_function = nn.MSELoss()\n",
    "# loss_function = nn.BCEWithLogitsLoss()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d649ab6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], loss_mean=2.703:   0%|          | 1/248 [01:12<4:59:58, 72.87s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m x_train = x_train.to(device)\n\u001b[32m     16\u001b[39m y_train = y_train.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m y_pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m loss = loss_function(y_pred, y_train)\n\u001b[32m     22\u001b[39m optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/DS/x-ray-pytorch/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/DS/x-ray-pytorch/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/DS/x-ray-pytorch/venv/lib/python3.13/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/DS/x-ray-pytorch/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/DS/x-ray-pytorch/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/DS/x-ray-pytorch/venv/lib/python3.13/site-packages/torch/nn/modules/activation.py:133\u001b[39m, in \u001b[36mReLU.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/DS/x-ray-pytorch/venv/lib/python3.13/site-packages/torch/nn/functional.py:1704\u001b[39m, in \u001b[36mrelu\u001b[39m\u001b[34m(input, inplace)\u001b[39m\n\u001b[32m   1702\u001b[39m     result = torch.relu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m   1703\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1704\u001b[39m     result = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1705\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "modal_start_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "os.makedirs(os.path.join(PROJECT_DIR_PATH, 'models',\n",
    "            modal_start_time), exist_ok=True)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "for _e in range(epochs):\n",
    "    loss_mean = 0  # среднее значение функции потерь (по эпохе)\n",
    "    lm_count = 0  # текущее количество слагаемых\n",
    "\n",
    "    train_tqdm = tqdm(train_data, leave=True)\n",
    "\n",
    "    for x_train, y_train in train_tqdm:\n",
    "\n",
    "        x_train = x_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "\n",
    "        y_pred = model(x_train)\n",
    "\n",
    "        loss = loss_function(y_pred, y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        lm_count += 1\n",
    "        loss_mean = 1/lm_count * loss.item() + (1 - 1/lm_count) * loss_mean\n",
    "        train_tqdm.set_description(\n",
    "            f\"Epoch [{_e+1}/{epochs}], loss_mean={loss_mean:.3f}\")\n",
    "\n",
    "    model_state_dict = {\n",
    "        'tfs': transform.state_dict(),\n",
    "        'opt': optimizer.state_dict(),\n",
    "        'model': model.state_dict(),\n",
    "    }\n",
    "\n",
    "    torch.save(model_state_dict, f\"./models/{modal_start_time}/epoch_{_e}.tar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9625d827",
   "metadata": {},
   "source": [
    "```py\n",
    "# weights_only=True означает, что выполняется загрузка примитивных типов данных, например: словарей, тензоров, списков, строк и т.п. \n",
    "model_data = torch.load('model_name.tar', weights_only=True)\n",
    "model.load_state_dict(model_data['model'])\n",
    "transforms.load_state_dict(model_data['tfs'])\n",
    "optimizer.load_state_dict(model_data['opt'])\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
